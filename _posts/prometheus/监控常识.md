监控原则

- 简单，保持最简单就好，业务系统全挂了，告警系统也不能挂
- 避免高magic系统， 避免ai运维，机器学习调阈值

prometheus的局限

- 基于metric监控，不适用日志，调用链与事件（需要多个有多维元素描述一个事件的，例如耗时交易时间+traceid+开始时间+结束时间四个变量才能定位一个有问题的交易事件）

- 采用的pull模型， 合理规划网络，避免转发

- prometheus的高可用与水平拓展， 官方社区也没有银弹，需要合理选择Federate、Cortex、Thanos等

- 监控系统的可用性大于一致性，容忍副本数据丢失，保证副本查询成功

- prometheus不一定保证数据准确，

  一是rate， histogram_quantile等函数会根据数据做统计和判断， 产生反直觉的结果。

  二是查询范围过长，要做降采样，势必会造成数据精度丢失，这就是时序数据的特点， 与日志系统不同的地方
  
- prometheus是单值数据模型，influxdb支持多值模型， 更适合做业务监控

  单个时间序列指标由一组label**值**相同的labelset与时间序列名指定，然后描述这一指标的value变化情况。当单个时间序列中需要将label**值**不同的labelset当做是同一个时间序列指标的时候，prometheus便不支持了。

  例如 execute_time{traceid=xxxx, uuid=xxxx, appname=xxx, srv=xxxx} value

  在上面这个指标中， traceid， uuid， value的值是会一直变的，由这三个值关联组成一条单独的时间序列，从业务角度理解，这个指标是execute_time{appname, srv} 有三个值，并且是一一对应有关联对应的，，但是prometheus的时间序列数据只能表示的是一个变化值，简单理解就是不支持外键。



指标选择原则

谷歌的四个黄金信号原则：延迟，流量， 错误率， 饱和度



prometheus 常遇到问题

- 大内存问题

  随着规模变大， prometheus所需的cpu与内存都会变大，内存一般会先存在瓶颈， 这个时候要么加大内存，要么集群分片，减少单机指标。

  原因：

  	1. prometheus每两个小时做一次落盘，落盘前所有的数据都保存在内存中，因此和采集量有关
  	2. 加载历史数据的时候，从磁盘加载到内存，查询范围越大， 内存需要的越大。这里面有一定优化空间
  	3. 一些不合理的查询条件， 如group或大范围的rate

  大概需要的内存预估方法：

  



1. 



相关技术讲解

Telegraf

​	采集组件all in one

​	prometheus中各个exporter都是独立运行的， 但是exporter越多运维压力越大，可以用Telegra来做all in one

Process-Exporter



 